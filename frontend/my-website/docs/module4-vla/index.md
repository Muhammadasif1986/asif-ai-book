# Module 4: Vision-Language-Action (VLA)

This module explores the cutting-edge field of Vision-Language-Action (VLA) models and their integration with humanoid robotics. Students will learn how to enable voice-to-action capabilities using technologies like Whisper, allowing robots to understand and respond to spoken commands. The module will cover cognitive planning approaches using Large Language Models (LLMs) to generate complex ROS 2 action pipelines. A capstone project will involve developing an autonomous humanoid robot capable of executing spoken tasks, bringing together concepts from all previous modules.

## Topics Covered

- Vision-Language-Action (VLA) model fundamentals
- Voice-to-action with Whisper
- Cognitive planning with LLMs for robotics
- LLM to ROS 2 action pipeline integration
- Capstone project: Autonomous humanoid executing spoken tasks

## Learning Objectives

- Understand the principles and applications of VLA models in robotics.
- Be able to implement voice control for humanoid robots.
- Design cognitive planning systems using LLMs.
- Integrate LLMs with ROS 2 for complex task execution.
- Develop an autonomous humanoid robot capable of understanding and performing spoken commands.
